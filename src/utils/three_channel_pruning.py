
# For each user query and triple list, we focus on (e1,r,?) and (?,r,e1), i.e., only focusing on head entity and relation or tail entity and relation, as well as (?,r/r',?) which only focuses on relations. We will have three recall channels:
# 1.(e1,r,?)
# 2.(?,r,e1)
# 3.(?,r/r',?)
# For these three recall channels, we need to recall the top k most relevant triples for each. Here k=100, so each query recalls at most 300 triples.
# 1.Then take the intersection of triples recalled by each user query to get the final pruned answer. Why not add the similarity scores of triples for each user query? Because the similarity scores recalled by these three channels may have large differences. But there are also cases where user query is related to all triples, for example, the "What is {topic entity}" generated by LLM is very blind, causing the recalled triples to be random, which will cause problems when taking intersection.
# 2.Or normalize the similarity scores of the three channels and then add them to the total score of triples, so the difference no longer exists. Then directly take the top k.
# task name is llm_pruning_three_channels

import sys
import os

sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/..")

from datasets import load_dataset
from llm.prompt_builder import *
from llm.llm_client import *
from tqdm import tqdm

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from utils.get_answer_entity_coverage import check_answer_in_graph_main
import torch
import pandas as pd
from datasets import Dataset, DatasetDict
from datetime import datetime

def three_channel_pruning(dataset_path=None,initial_pruning_llm="sentence-transformers",embedding_model_path=None,llm_pruning_topk=100,resume_path=None):

    ###############################################################################################################
    dataset_name = "cwq"
    if "cwq" in dataset_path:
        dataset_name = "cwq"
    elif "webqsp" in dataset_path:
        dataset_name = "webqsp"

    dataset = load_dataset("parquet", data_files={'test': dataset_path})

    # Print dataset information
    print(dataset)
    
    # Get current time
    current_time = datetime.now()

    # Format time as string (e.g., "2023-10-10_14-30-00"), replace this with the name of the unfinished parquet from last time to continue
    time_str = current_time.strftime("%Y-%m-%d_%H-%M-%S")

    # Save file, final output file name is {dataset_name}_{llm}_{qa}
    if resume_path == None:
        write_data_dir = f"preprocess_datasets/llm_pruning_three_channels_datasets/{dataset_name}_{initial_pruning_llm}_{llm_pruning_topk}_llm_pruning_three_channels_{time_str}.parquet"
    else:
        write_data_dir = resume_path

    # Open the file, if it doesn't exist, create it
    # Ensure directory exists
    os.makedirs(os.path.dirname(write_data_dir), exist_ok=True)

    # Store results after question decomposition
    llm_pruning_three_channels_dataset = None
    finished_id = []

    # Check if file exists, if not, create file
    if not os.path.exists(write_data_dir):
        # If file doesn't exist, create an empty DataFrame and save as parquet file
        df = pd.DataFrame()  # Create empty DataFrame
        df.to_parquet(write_data_dir)
        print(f"File doesn't exist, created new empty file: {write_data_dir}")
        # Initialize dataset
        llm_pruning_three_channels_dataset = DatasetDict({
            "test": Dataset.from_dict({
                "id": "",
                "question": "",
                "user_queries":[],
                "answer": [],
                "q_entity": [],
                "a_entity": [], 
                "graph": [],
                "pruned_graph": [],
                "choices": []
            })
        })
    else:
        print(f"File already exists: {write_data_dir}, will continue llm three channels pruning task from this file")
        llm_pruning_three_channels_dataset = load_dataset("parquet", data_files={'test': write_data_dir})
        # Check existing question_id
        for sample in llm_pruning_three_channels_dataset["test"]:
            finished_id.append(sample["id"])

    ###############################################################################################################
    # MPS > CUDA > CPU
    if torch.backends.mps.is_available() and torch.backends.mps.is_built():
        device = "mps"
    elif torch.cuda.is_available():
        device = "cuda"
    else:
        device = "cpu"
    print(f"Using device: {device}")
    
    # Initialize Sentence-BERT model
    model = SentenceTransformer(embedding_model_path,device=device)

    print(f"Coverage information for {dataset_name} dataset before LLM pruning:")
    check_answer_in_graph_main(dataset=dataset,graph_name="graph")

    filtered_graph = {}
    # Convert dataset["test"] to a fast retrieval dictionary with id as key
    id_to_example_map = {example["id"]: example for example in dataset["test"]}

    # List for temporarily storing results
    batch_results = []
    filter_batch_size = 30  # Set batch size

    for sample in tqdm(dataset["test"], desc="LLM three channels pruning"):

        # Three channels
        corpus1 = []
        corpus2 = []
        corpus3 = []

        # Skip if already pruned
        if sample["id"] in finished_id:
            continue
        if llm_pruning_topk >= len(sample["graph"]):
            filtered_graph[sample["id"]] = sample["graph"]
            question_id = sample["id"]
            # Quickly retrieve corresponding record by question_id
            example = id_to_example_map.get(question_id)
            if example:
                # Add prediction result to example
                example_with_prediction = {**example, "pruned_graph": sample["graph"]}
                batch_results.append(example_with_prediction)
        else:
            # Convert each element to string and store in corpus
            # Channel 1
            for sublist in sample["graph"]:
                corpus1.append(" ".join(map(str, sublist[:2])))
            corpus_embeddings1 = model.encode(corpus1)
            # Channel 2
            for sublist in sample["graph"]:
                corpus2.append(" ".join(map(str, sublist[-2:])))
            corpus_embeddings2 = model.encode(corpus2)
            # Channel 3
            for sublist in sample["graph"]:
                corpus3.append("".join(map(str, sublist[1])))
            # Embed the candidate set
            corpus_embeddings3 = model.encode(corpus3)

            # Normalize embedding vectors (cosine similarity requires normalizing vectors to unit norm)
            corpus_embeddings1 = corpus_embeddings1 / np.linalg.norm(corpus_embeddings1, axis=1, keepdims=True)
            corpus_embeddings2 = corpus_embeddings2 / np.linalg.norm(corpus_embeddings2, axis=1, keepdims=True)
            corpus_embeddings3 = corpus_embeddings3 / np.linalg.norm(corpus_embeddings3, axis=1, keepdims=True)

            # Build FAISS index (using inner product)
            embedding_dim = corpus_embeddings1.shape[1]  # Dimension of embedding vectors
            index1 = faiss.IndexFlatIP(embedding_dim)    # Use inner product
            index1.add(corpus_embeddings1)               # Add embedding vectors to index
            index2 = faiss.IndexFlatIP(embedding_dim)    # Use inner product
            index2.add(corpus_embeddings2)
            index3 = faiss.IndexFlatIP(embedding_dim)    # Use inner product
            index3.add(corpus_embeddings3)

            # Calculate scores for each query in sample["user_queries"]
            total_scores = np.zeros(len(corpus1))  # Store total score for each corpus

            all_queries = []
            all_queries.append(sample["question"])
            for query in sample["user_queries"]:
                all_queries.append(query)
            for query in all_queries:
                query_embedding = model.encode([query])  # Embed query

                # Normalize query vector
                query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)

                # Channel 1
                distances1, indices1 = index1.search(query_embedding, len(corpus1))
                # Channel 2
                distances2, indices2 = index2.search(query_embedding, len(corpus2))
                # Channel 3
                distances3, indices3 = index3.search(query_embedding, len(corpus3))

                # Normalize scores for each channel
                normalized_scores1 = (distances1[0] - distances1[0].min()) / (distances1[0].max() - distances1[0].min() + 1e-9)
                normalized_scores2 = (distances2[0] - distances2[0].min()) / (distances2[0].max() - distances2[0].min() + 1e-9)
                normalized_scores3 = (distances3[0] - distances3[0].min()) / (distances3[0].max() - distances3[0].min() + 1e-9)

                # View case
                # print("Channel 1:")
                # for i, idx in enumerate(indices1[0][:llm_pruning_topk]):  # Iterate through returned indices
                #     print(f"No.{i}:",sample["graph"][idx],f"score:{normalized_scores1[i]}")

                # print("Channel 2:")
                # for i, idx in enumerate(indices2[0][:llm_pruning_topk]):  # Iterate through returned indices
                #     print(f"No.{i}:",sample["graph"][idx],f"score:{normalized_scores2[i]}")

                # print("Channel 3:")
                # for i, idx in enumerate(indices3[0][:llm_pruning_topk]):  # Iterate through returned indices
                #     print(f"No.{i}:",sample["graph"][idx],f"score:{normalized_scores3[i]}")

                # Add channel 1 scores to corresponding positions in total_scores
                for i, idx in enumerate(indices1[0]):  # Iterate through returned indices
                    total_scores[idx] += normalized_scores1[i]

                for i, idx in enumerate(indices2[0]):  # Iterate through returned indices
                    total_scores[idx] += normalized_scores2[i]

                for i, idx in enumerate(indices3[0]):  # Iterate through returned indices
                    total_scores[idx] += normalized_scores3[i]
                # Accumulate normalized scores to total scores
                # total_scores += normalized_scores1
                # total_scores += normalized_scores2
                # total_scores += normalized_scores3

            # Sort by total score in descending order and select top k, using llm_pruning_topk
            top_k_indices = np.argsort(total_scores)[::-1][:llm_pruning_topk]

            # Filtered triple list
            triple_filtered_graph = [sample["graph"][idx] for idx in top_k_indices]

            filtered_graph[sample["id"]] = triple_filtered_graph

            question_id = sample["id"]
            processed_answer = triple_filtered_graph
            # Quickly retrieve corresponding record by question_id
            example = id_to_example_map.get(question_id)
            if example:
                # Add prediction result to example
                example_with_prediction = {**example, "pruned_graph": processed_answer}
                batch_results.append(example_with_prediction)

            # When batch results reach filter_batch_size, perform one write
            if len(batch_results) >= filter_batch_size:
                if len(llm_pruning_three_channels_dataset["test"]) == 0:  # If llm_pruning_three_channels_dataset["test"] is empty
                    llm_pruning_three_channels_dataset["test"] = Dataset.from_dict({key: [ex[key] for ex in batch_results] for key in batch_results[0]})
                else:
                    # Batch merge to existing Dataset
                    llm_pruning_three_channels_dataset["test"] = Dataset.from_dict({
                        key: llm_pruning_three_channels_dataset["test"][key] + [ex[key] for ex in batch_results]
                        for key in llm_pruning_three_channels_dataset["test"].column_names
                    })

                # Write to file
                llm_pruning_three_channels_dataset["test"].to_parquet(write_data_dir)
                batch_results = []  # Clear temporary storage

    # If there are remaining results, write to file
    if batch_results:
        if len(llm_pruning_three_channels_dataset["test"]) == 0:  # If llm_pruning_three_channels_dataset["test"] is empty
            llm_pruning_three_channels_dataset["test"] = Dataset.from_dict({key: [ex[key] for ex in batch_results] for key in batch_results[0]})
        else:
            llm_pruning_three_channels_dataset["test"] = Dataset.from_dict({
                key: llm_pruning_three_channels_dataset["test"][key] + [ex[key] for ex in batch_results]
                for key in llm_pruning_three_channels_dataset["test"].column_names
            })
        llm_pruning_three_channels_dataset["test"].to_parquet(write_data_dir)
    
    print(f"Completed llm three channels pruning task for {dataset_name} dataset!")

    # Check the length of subgraph after coverage and answer coverage rate
    pruned_subgraph_total_length = 0
    for sample in dataset["test"]:
        pruned_subgraph_total_length = pruned_subgraph_total_length + len(sample["graph"])

    print(f"Coverage information for {dataset_name} dataset after pruning:")
    check_answer_in_graph_main(dataset=llm_pruning_three_channels_dataset,graph_name="pruned_graph")
